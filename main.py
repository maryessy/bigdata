import streamlit as st
import pandas as pd
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
# --- NUEVO: Importar MLPRegressor de scikit-learn ---
from sklearn.neural_network import MLPRegressor
# --- FIN NUEVO ---
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# --- Configuraci칩n de la P치gina ---
st.set_page_config(
    page_title="Explorador de Modelos Predictivos",
    page_icon="游뱄",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- Carga y Cacheo de Datos ---
@st.cache_data
def load_data():
    """
    Carga el dataset de California Housing, lo divide en conjuntos de entrenamiento y prueba,
    y escala las caracter칤sticas.
    """
    housing = fetch_california_housing()
    df = pd.DataFrame(housing.data, columns=housing.feature_names)
    df['MedHouseVal'] = housing.target
    
    # Scikit-learn a partir de la versi칩n 1.2 ya no incluye lat y lon en .data
    # Esta l칩gica asegura que se asignen correctamente.
    df['Latitude'] = housing.data[:, -2] if 'Latitude' not in housing.feature_names else df['Latitude']
    df['Longitude'] = housing.data[:, -1] if 'Longitude' not in housing.feature_names else df['Longitude']

    # Asegurar que las caracter칤sticas (X) no incluyan latitud, longitud ni el target.
    feature_names_to_use = [name for name in housing.feature_names if name not in ['Latitude', 'Longitude']]
    X = df[feature_names_to_use]
    y = df['MedHouseVal']
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Escalar caracter칤sticas es una buena pr치ctica para muchos modelos, especialmente redes neuronales
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Devolvemos tanto los datos escalados como los no escalados y el scaler
    return df, X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled, scaler, feature_names_to_use

# --- Entrenamiento de Modelos (Cacheado) ---
@st.cache_resource
def train_model(model_name, X_train, y_train, params):
    """
    Entrena un modelo predictivo con los hiperpar치metros dados.
    Utiliza st.cache_resource para evitar re-entrenar en cada rerun.
    """
    if model_name == 'Regresi칩n Lineal':
        model = LinearRegression()
    elif model_name == '츼rbol de Decisi칩n':
        model = DecisionTreeRegressor(max_depth=params['max_depth'], min_samples_split=params['min_samples_split'], random_state=42)
    elif model_name == 'Random Forest':
        model = RandomForestRegressor(n_estimators=params['n_estimators'], max_depth=params['max_depth'], random_state=42)
    elif model_name == 'Gradient Boosting':
        model = GradientBoostingRegressor(n_estimators=params['n_estimators'], learning_rate=params['learning_rate'], max_depth=params['max_depth'], random_state=42)
    elif model_name == 'Support Vector Regressor (SVR)':
        model = SVR(C=params['C'], kernel=params['kernel'], gamma=params['gamma'])
    # --- NUEVO: Implementaci칩n de MLPRegressor ---
    elif model_name == 'Red Neuronal (MLPRegressor)':
        # hidden_layer_sizes acepta una tupla, por ejemplo (100,) para una capa, (50, 50) para dos.
        # En este caso, lo hacemos simple con una o dos capas ajustables.
        hidden_layers = tuple(params['hidden_layer_sizes'])
        model = MLPRegressor(
            hidden_layer_sizes=hidden_layers,
            activation=params['activation'],
            solver='adam', # Adam es un buen optimizador por defecto
            alpha=params['alpha'], # Par치metro de regularizaci칩n L2
            learning_rate_init=params['learning_rate_init'],
            max_iter=params['max_iter'], # N칰mero m치ximo de 칠pocas
            random_state=42,
            early_stopping=True, # Detener si el rendimiento no mejora en validaci칩n
            n_iter_no_change=10, # N칰mero de 칠pocas sin mejora para activar early stopping
            tol=1e-4 # Tolerancia para la mejora
        )
    # --- FIN NUEVO ---
    
    model.fit(X_train, y_train)
    return model

# --- Cargar todos los datos al inicio ---
df, X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled, scaler, feature_names = load_data()


# --- Barra Lateral (Sidebar) ---
st.sidebar.title("Panel de Navegaci칩n")
# --- NUEVO: A침adir "Red Neuronal (MLPRegressor)" a la lista de opciones ---
app_mode = st.sidebar.selectbox(
    "Selecciona una secci칩n:",
    ["Introducci칩n", "An치lisis Exploratorio de Datos (EDA)", "Regresi칩n Lineal", 
     "츼rbol de Decisi칩n", "Random Forest", "Gradient Boosting", "Support Vector Regressor (SVR)",
     "Red Neuronal (MLPRegressor)"] # Nueva opci칩n
)
# --- FIN NUEVO ---

st.sidebar.divider()

# CORRECCI칍N: Usar session_state para simular un modal y evitar el error de atributo.
# 1. Inicializar el estado si no existe.
if 'show_info_modal' not in st.session_state:
    st.session_state.show_info_modal = False

# 2. Bot칩n para ABRIR la gu칤a te칩rica.
if st.sidebar.button("Consultar info de Modelos", use_container_width=True):
    st.session_state.show_info_modal = True


# --- Contenedor de Informaci칩n Te칩rica (simula un modal) ---
# 3. Mostrar el contenido si el estado es True.
if st.session_state.show_info_modal:
    with st.expander("Gu칤a Te칩rica de Modelos Predictivos e Hiperpar치metros", expanded=True):
        st.markdown("""
        ## Gu칤a Te칩rica de Modelos e Hiperpar치metros

        Esta gu칤a te ayudar치 a entender los conceptos clave detr치s de los modelos utilizados en esta aplicaci칩n.

        ---
        
        ### 쯈u칠 es un Hiperpar치metro?
        
        En Machine Learning, un **hiperpar치metro** es una configuraci칩n externa al modelo cuyo valor no se puede aprender de los datos. Es una perilla que el cient칤fico de datos ajusta *antes* de entrenar el modelo para controlar su comportamiento y rendimiento. La elecci칩n correcta de hiperpar치metros es crucial y a menudo requiere experimentaci칩n.

        ---

        ### 1. Regresi칩n Lineal

        - **Teor칤a:** Es el modelo m치s simple. Busca encontrar la mejor l칤nea recta (o hiperplano en m치s dimensiones) que se ajuste a los datos. Intenta modelar la relaci칩n entre las variables de entrada (X) y la variable de salida (y) mediante una ecuaci칩n lineal.
        - **F칩rmula:** $$ y = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_nx_n + \epsilon $$
        - **Hiperpar치metros en esta app:** Ninguno. La Regresi칩n Lineal est치ndar no tiene hiperpar치metros significativos que ajustar, lo que la hace un excelente punto de partida (baseline).

        ---

        ### 2. 츼rbol de Decisi칩n

        - **Teor칤a:** Funciona creando un modelo similar a un 치rbol de reglas de "si-entonces" que segmentan los datos. En cada nodo, el 치rbol elige la mejor caracter칤stica para dividir los datos y as칤 minimizar el error en las predicciones.
        - **Hiperpar치metros en esta app:**
            - `max_depth` (Profundidad M치xima): Controla cu치n profundo puede crecer el 치rbol. Un valor bajo puede causar subajuste (modelo demasiado simple), mientras que un valor muy alto puede causar sobreajuste (el modelo memoriza los datos).
            - `min_samples_split` (M칤nimo de Muestras para Dividir): Es el n칰mero m칤nimo de muestras que un nodo debe tener para poder ser dividido. Ayuda a prevenir el sobreajuste al no permitir divisiones en nodos con muy pocos datos.

        ---

        ### 3. Random Forest (Bosque Aleatorio)

        - **Teor칤a:** Es un m칠todo de *ensamble* que mejora la precisi칩n y controla el sobreajuste al entrenar m칰ltiples 츼rboles de Decisi칩n. Cada 치rbol se entrena con una muestra aleatoria de los datos y de las caracter칤sticas. La predicci칩n final es el promedio de las predicciones de todos los 치rboles individuales.
        - **Hiperpar치metros en esta app:**
            - `n_estimators` (N칰mero de 츼rboles): La cantidad de 치rboles que se construir치n en el bosque. Generalmente, m치s 치rboles mejoran el rendimiento, pero a un costo computacional mayor.
            - `max_depth`: Al igual que en un 치rbol de decisi칩n individual, controla la profundidad m치xima de cada 치rbol en el bosque.

        ---

        ### 4. Gradient Boosting

        - **Teor칤a:** Es otro m칠todo de *ensamble* que construye modelos (generalmente 치rboles) de forma secuencial. A diferencia de Random Forest, cada nuevo 치rbol se entrena para corregir los errores cometidos por los 치rboles anteriores. Es un m칠todo muy potente que a menudo resulta en modelos de alto rendimiento.
        - **Hiperpar치metros en esta app:**
            - `n_estimators`: El n칰mero de 치rboles (etapas de boosting) que se realizar치n.
            - `learning_rate` (Tasa de Aprendizaje): Reduce la contribuci칩n de cada 치rbol. Valores m치s bajos requieren m치s 치rboles para el mismo rendimiento, pero a menudo resultan en una mejor generalizaci칩n. Su valor est치 entre 0.0 y 1.0.
            - `max_depth`: La profundidad m치xima de cada 치rbol individual.

        ---

        ### 5. Support Vector Regressor (SVR)

        - **Teor칤a:** A diferencia de otros modelos que intentan minimizar el error, SVR intenta ajustar la mayor cantidad de datos posible *dentro* de un margen o "tubo" definido por un hiperpar치metro 칠psilon (no ajustable en esta app). Los puntos fuera de este tubo son penalizados. Es muy efectivo en espacios de caracter칤sticas de alta dimensi칩n.
        - **Hiperpar치metros en esta app:**
            - `C` (Par치metro de Regularizaci칩n): Controla la penalizaci칩n por los puntos que quedan fuera del margen. Un valor alto de `C` intenta ajustar m치s datos correctamente (menos errores), arriesg치ndose a un sobreajuste. Un valor bajo permite m치s errores (un margen m치s "suave").
            - `kernel`: Define la funci칩n utilizada para transformar los datos a una dimensi칩n superior y encontrar la mejor relaci칩n.
                - `linear`: Para relaciones lineales.
                - `poly`: Para relaciones polin칩micas.
                - `rbf` (Radial Basis Function): Muy popular y flexible para relaciones no lineales complejas. Su f칩rmula es: $$ K(x_i, x_j) = \exp(-\gamma ||x_i - x_j||^2) $$
            - `gamma`: Un par치metro del kernel `rbf`. Define cu치nta influencia tiene un solo ejemplo de entrenamiento. Valores bajos significan una influencia lejana, mientras que valores altos significan una influencia cercana.
        
        ---
        
        ### 6. Red Neuronal (MLPRegressor de Scikit-learn)
        
        - **Teor칤a:** El `MLPRegressor` es una red neuronal artificial de tipo *Perceptr칩n Multicapa* implementada en scikit-learn. Consiste en una capa de entrada, una o m치s capas ocultas y una capa de salida. Cada "neurona" en las capas ocultas aplica una funci칩n de activaci칩n (como ReLU o Sigmoide) a una suma ponderada de sus entradas, permitiendo al modelo aprender relaciones no lineales complejas en los datos.
        - **Hiperpar치metros en esta app:**
            - `hidden_layer_sizes` (Tama침os de Capas Ocultas): Define la arquitectura de las capas ocultas. Puedes especificar el n칰mero de neuronas para cada capa oculta. Por ejemplo, `(100,)` significa una capa oculta con 100 neuronas, y `(50, 20)` significa dos capas ocultas, la primera con 50 neuronas y la segunda con 20.
            - `activation` (Funci칩n de Activaci칩n): La funci칩n de activaci칩n para las capas ocultas.
                - `relu`: Funci칩n de activaci칩n por defecto, `f(x) = max(0, x)`. Es muy com칰n y eficiente.
                - `logistic`: La funci칩n sigmoide, `f(x) = 1 / (1 + exp(-x))`. Comprime la salida entre 0 y 1.
                - `tanh`: Funci칩n tangente hiperb칩lica, `f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))`. Comprime la salida entre -1 y 1.
            - `alpha` (T칠rmino de Regularizaci칩n L2): Un par치metro de regularizaci칩n que agrega una penalizaci칩n al tama침o de los pesos para evitar el sobreajuste. Valores m치s altos aumentan la regularizaci칩n.
            - `learning_rate_init` (Tasa de Aprendizaje Inicial): El tama침o de los pasos iniciales que toma el optimizador al ajustar los pesos. Una tasa alta puede hacer que el entrenamiento sea inestable; una baja puede ralentizarlo.
            - `max_iter` (M치ximo de Iteraciones/칄pocas): El n칰mero m치ximo de 칠pocas (pasadas completas sobre el dataset de entrenamiento) que el algoritmo realizar치. El entrenamiento puede detenerse antes si se activa el `early_stopping`.

        """)
        # 4. Bot칩n para CERRAR la gu칤a.
        if st.button("Cerrar Gu칤a"):
            st.session_state.show_info_modal = False
            st.rerun() # Fuerza un rerun para que el expander desaparezca inmediatamente.

# --- L칩gica de P치ginas ---
if app_mode == "Introducci칩n":
    st.title("游뱄 Explorador Interactivo de Modelos Predictivos")
    st.markdown("""
    춰Bienvenido a esta aplicaci칩n interactiva para explorar, entrenar y evaluar diversos modelos predictivos de Machine Learning!
    
    Esta herramienta est치 dise침ada como una pr치ctica para entender los conceptos de modelado en un contexto de Big Data, utilizando el famoso dataset **"California Housing"**. El objetivo es predecir el **valor mediano de una vivienda** en California basado en diferentes caracter칤sticas.

    ### 쯈u칠 puedes hacer aqu칤?
    
    1.  **Navegar por las Secciones:** Utiliza el men칰 en el panel lateral para moverte entre las diferentes partes de la aplicaci칩n.
    2.  **An치lisis Exploratorio de Datos (EDA):** Entiende la estructura y distribuci칩n de los datos con tablas y visualizaciones.
    3.  **Explorar Modelos Predictivos:**
        - Selecciona un modelo de la lista (Regresi칩n Lineal, 츼rbol de Decisi칩n, etc.).
        - Lee una breve explicaci칩n sobre c칩mo funciona cada modelo.
        - **Ajusta los hiperpar치metros** del modelo usando los controles interactivos en el panel lateral.
        - Observa c칩mo cambian las m칠tricas de rendimiento (Error Cuadr치tico Medio y R) en tiempo real.
    4.  **Realizar Predicciones:** En la secci칩n de cada modelo, puedes introducir tus propios valores para las caracter칤sticas de una vivienda y obtener una predicci칩n instant치nea del precio.

    **춰Comienza explorando el dataset en la secci칩n de "An치lisis Exploratorio de Datos (EDA)" o salta directamente a entrenar un modelo!**
    """)

elif app_mode == "An치lisis Exploratorio de Datos (EDA)":
    st.title("游늵 An치lisis Exploratorio de Datos (EDA)")
    st.header("Dataset: California Housing")
    st.markdown("Este dataset contiene informaci칩n de los grupos de bloques censales en California. Cada fila corresponde a un grupo de bloques.")
    
    st.subheader("Vistazo a los Datos")
    st.dataframe(df.head())
    
    st.subheader("Descripci칩n de las Caracter칤sticas")
    st.markdown("""
    - **MedInc:** Ingreso mediano en el grupo de bloques (en decenas de miles de d칩lares).
    - **HouseAge:** Antig칲edad mediana de las viviendas en el grupo de bloques.
    - **AveRooms:** N칰mero promedio de habitaciones por vivienda.
    - **AveBedrms:** N칰mero promedio de dormitorios por vivienda.
    - **Population:** Poblaci칩n del grupo de bloques.
    - **AveOccup:** Ocupaci칩n promedio por vivienda (miembros por hogar).
    - **Latitude:** Latitud del centro del grupo de bloques.
    - **Longitude:** Longitud del centro del grupo de bloques.
    - **MedHouseVal (Target):** Valor mediano de la vivienda en el grupo de bloques (en cientos de miles de d칩lares).
    """)

    st.subheader("Descripci칩n Estad칤stica de los Datos")
    st.write(df.describe())

    st.subheader("Visualizaciones")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### Distribuci칩n del Valor Mediano de la Vivienda (Target)")
        st.markdown("El valor est치 en cientos de miles de d칩lares ($100,000s).")
        st.bar_chart(df['MedHouseVal'].value_counts().sort_index())
    
    with col2:
        st.markdown("#### Ubicaci칩n Geogr치fica de las Viviendas")
        st.markdown("El color representa el valor mediano de la vivienda (m치s oscuro = m치s caro).")
        
        map_df = df.copy()
        
        min_val = map_df['MedHouseVal'].min()
        max_val = map_df['MedHouseVal'].max()
        map_df['alpha'] = ((map_df['MedHouseVal'] - min_val) / (max_val - min_val)) * 255
        
        map_df['color'] = map_df['alpha'].apply(lambda alpha: [255, 90, 0, int(alpha)])
        
        map_df['size'] = map_df['Population'] * 0.01

        st.map(map_df, latitude='Latitude', longitude='Longitude', color='color', size='size')

else:
    model_name = app_mode
    st.title(f"游뱄 Modelo Predictivo: {model_name}")

    model_descriptions = {
        "Regresi칩n Lineal": "Un modelo simple que busca la mejor relaci칩n lineal entre las caracter칤sticas y el valor de la vivienda. Es r치pido y f치cil de interpretar, pero puede no capturar relaciones complejas.",
        "츼rbol de Decisi칩n": "Este modelo aprende una serie de reglas de 'si-entonces' para dividir los datos y hacer una predicci칩n. Es muy interpretable, pero propenso a sobreajustarse (memorizar) a los datos de entrenamiento.",
        "Random Forest": "Un modelo de ensamble que entrena muchos 츼rboles de Decisi칩n sobre diferentes subconjuntos de datos y promedia sus predicciones. Es mucho m치s robusto y preciso que un solo 치rbol, pero menos interpretable.",
        "Gradient Boosting": "Otro modelo de ensamble que construye 치rboles de forma secuencial. Cada nuevo 치rbol intenta corregir los errores de los 치rboles anteriores. Suele ser uno de los modelos de mayor rendimiento.",
        "Support Vector Regressor (SVR)": "Busca encontrar un 'tubo' que contenga la mayor cantidad de puntos de datos posible, minimizando las desviaciones fuera de 칠l. Es muy efectivo en espacios de alta dimensi칩n y cuando las relaciones no son lineales, especialmente con el kernel 'rbf'.",
        # --- NUEVO: Descripci칩n para MLPRegressor ---
        "Red Neuronal (MLPRegressor)": "Una implementaci칩n de red neuronal multicapa (Perceptr칩n Multicapa) por scikit-learn. Aprende relaciones complejas y no lineales en los datos. Requiere datos escalados para un rendimiento 칩ptimo."
        # --- FIN NUEVO ---
    }
    st.markdown(model_descriptions.get(model_name, "Selecciona un modelo para ver su descripci칩n."))
    st.divider()

    st.sidebar.header("Ajuste de Hiperpar치metros")
    params = {}
    if model_name == '츼rbol de Decisi칩n':
        params['max_depth'] = st.sidebar.slider("Profundidad M치xima (max_depth)", 1, 30, 10)
        params['min_samples_split'] = st.sidebar.slider("M칤nimo de Muestras para Dividir (min_samples_split)", 2, 100, 2)
    elif model_name == 'Random Forest':
        params['n_estimators'] = st.sidebar.slider("N칰mero de 츼rboles (n_estimators)", 10, 500, 100, step=10)
        params['max_depth'] = st.sidebar.slider("Profundidad M치xima (max_depth)", 1, 30, 10)
    elif model_name == 'Gradient Boosting':
        params['n_estimators'] = st.sidebar.slider("N칰mero de 츼rboles (n_estimators)", 10, 500, 100, step=10)
        params['learning_rate'] = st.sidebar.slider("Tasa de Aprendizaje (learning_rate)", 0.01, 1.0, 0.1, step=0.01)
        params['max_depth'] = st.sidebar.slider("Profundidad M치xima (max_depth)", 1, 10, 3)
    elif model_name == 'Support Vector Regressor (SVR)':
        params['C'] = st.sidebar.slider("Par치metro de Regularizaci칩n (C)", 0.01, 10.0, 1.0)
        params['kernel'] = st.sidebar.selectbox("Kernel", ['linear', 'rbf', 'poly'])
        params['gamma'] = st.sidebar.select_slider("Gamma", options=['scale', 'auto', 0.01, 0.1, 1])
    # --- NUEVO: Hiperpar치metros para MLPRegressor ---
    elif model_name == 'Red Neuronal (MLPRegressor)':
        st.sidebar.markdown("##### Capas Ocultas")
        num_layers = st.sidebar.slider("N칰mero de Capas Ocultas", 1, 3, 1)
        hidden_layer_sizes = []
        for i in range(num_layers):
            size = st.sidebar.slider(f"Neuronas Capa {i+1}", 10, 200, 100, step=10, key=f"mlp_neurons_{i}")
            hidden_layer_sizes.append(size)
        params['hidden_layer_sizes'] = tuple(hidden_layer_sizes)

        params['activation'] = st.sidebar.selectbox("Funci칩n de Activaci칩n", ['relu', 'logistic', 'tanh'])
        params['alpha'] = st.sidebar.slider("Regularizaci칩n L2 (alpha)", 0.0001, 0.1, 0.0001, format="%.4f")
        params['learning_rate_init'] = st.sidebar.slider("Tasa de Aprendizaje Inicial", 0.0001, 0.01, 0.001, format="%.4f")
        params['max_iter'] = st.sidebar.slider("M치ximo de 칄pocas (max_iter)", 50, 1000, 200, step=50)
    # --- FIN NUEVO ---

    # --- NUEVO: Usar X_train_scaled y X_test_scaled para SVR y MLPRegressor ---
    # Los modelos basados en redes neuronales (como MLPRegressor) y SVR son sensibles a la escala de los datos.
    X_train_to_use = X_train_scaled if model_name in ['Support Vector Regressor (SVR)', 'Red Neuronal (MLPRegressor)'] else X_train
    X_test_to_use = X_test_scaled if model_name in ['Support Vector Regressor (SVR)', 'Red Neuronal (MLPRegressor)'] else X_test
    # --- FIN NUEVO ---

    with st.spinner("Entrenando el modelo con los par치metros seleccionados..."):
        model = train_model(model_name, X_train_to_use, y_train, params)
    st.success("춰Modelo entrenado exitosamente!")

    y_pred = model.predict(X_test_to_use)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    st.header("Evaluaci칩n del Modelo")
    col1, col2 = st.columns(2)
    with col1:
        st.metric("Error Cuadr치tico Medio (MSE)", f"{mse:.4f}")
        st.info("游눠 **MSE:** Mide el promedio de los errores al cuadrado. Un valor m치s bajo es mejor.")
    with col2:
        st.metric("Coeficiente de Determinaci칩n (R)", f"{r2:.4f}")
        st.info("游눠 **R:** Indica qu칠 proporci칩n de la varianza en el valor de la vivienda es predecible a partir de las caracter칤sticas. Un valor m치s cercano a 1 es mejor.")

    st.divider()
    st.header("Realizar una Predicci칩n en Vivo")
    st.markdown("Ajusta los siguientes valores para simular una nueva vivienda y predecir su precio.")
    
    input_data = {}
    prediction_cols = st.columns(2)
    
    for i, feature in enumerate(feature_names):
        with prediction_cols[i % 2]:
            mean_val = df[feature].mean()
            std_val = df[feature].std()
            min_val = df[feature].min()
            max_val = df[feature].max()
            
            input_data[feature] = st.slider(
                f"Valor para **{feature}**", 
                min_value=float(min_val),
                max_value=float(max_val),
                value=float(mean_val),
                step=float(std_val / 10)
            )

    if st.button("Predecir Valor de la Vivienda"):
        input_df = pd.DataFrame([input_data])
        
        # --- NUEVO: Manejo de escalado para SVR y MLPRegressor ---
        if model_name in ['Support Vector Regressor (SVR)', 'Red Neuronal (MLPRegressor)']:
            input_df_scaled = scaler.transform(input_df)
            prediction = model.predict(input_df_scaled)
        else:
            prediction = model.predict(input_df)
        # --- FIN NUEVO ---
        
        st.subheader("Resultado de la Predicci칩n")
        st.success(f"El valor mediano estimado para la vivienda es: **${prediction[0] * 100000:,.2f}**")

    with st.expander("Ver el c칩digo de Python para este modelo"):
        # --- NUEVO: Generaci칩n de c칩digo para MLPRegressor ---
        if model_name == 'Red Neuronal (MLPRegressor)':
            code = f"""
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Nota: X_train_scaled, y_train, X_test_scaled, y_test son los datos pre-divididos y escalados.

# 1. Inicializar el modelo MLPRegressor con los par치metros seleccionados
params = {params}
# hidden_layer_sizes debe ser una tupla, aseguramos que lo sea.
model = MLPRegressor(
    hidden_layer_sizes={tuple(params['hidden_layer_sizes'])},
    activation='{params['activation']}',
    solver='adam', 
    alpha={params['alpha']}, 
    learning_rate_init={params['learning_rate_init']},
    max_iter={params['max_iter']}, 
    random_state=42,
    early_stopping=True,
    n_iter_no_change=10,
    tol=1e-4
)

# 2. Entrenar el modelo (usando datos escalados)
model.fit(X_train_scaled, y_train)

# 3. Realizar predicciones en el conjunto de prueba (usando datos escalados)
y_pred = model.predict(X_test_scaled)

# 4. Evaluar el rendimiento
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"MSE: {{mse:.4f}}")
print(f"R: {{r2:.4f}}")
"""
        else:
            code = f"""
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.{'linear_model' if model_name == 'Regresi칩n Lineal' else 'tree' if model_name == '츼rbol de Decisi칩n' else 'ensemble' if model_name in ['Random Forest', 'Gradient Boosting'] else 'svm'}.{model.__class__.__name__}
from sklearn.metrics import mean_squared_error, r2_score

# Nota: X_train, y_train, X_test, y_test son los datos pre-divididos.
# Para SVR, se usar칤an X_train_scaled y X_test_scaled.

# 1. Inicializar el modelo con los par치metros seleccionados
params = {params}
model = {model.__class__.__name__}(**params, random_state=42) if 'random_state' in {model.__class__}().get_params() else {model.__class__.__name__}(**params)

# 2. Entrenar el modelo
model.fit(X_train, y_train)

# 3. Realizar predicciones en el conjunto de prueba
y_pred = model.predict(X_test)

# 4. Evaluar el rendimiento
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"MSE: {{mse:.4f}}")
print(f"R: {{r2:.4f}}")
"""
        # --- FIN NUEVO ---
        st.code(code, language='python')
